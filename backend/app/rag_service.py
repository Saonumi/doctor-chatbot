import os
from dotenv import load_dotenv

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_text_splitters.character import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains import create_retrieval_chain
from langchain_core.output_parsers import StrOutputParser

# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘á»ƒ trÃ¡nh lá»—i path
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
VECTOR_DB_PATH = os.path.join(BASE_DIR, "storage", "vector_db")
INDEX_NAME = "tcm_index"

class RAGService:
    def __init__(self):
        # 1. Khá»Ÿi táº¡o model Embeddings Local (Miá»…n phÃ­, khÃ´ng giá»›i háº¡n)
        # Sá»­ dá»¥ng model há»— trá»£ Ä‘a ngÃ´n ngá»¯ (bao gá»“m tiáº¿ng Viá»‡t)
        print("ğŸ“¥ Äang táº£i/load model embedding local (láº§n Ä‘áº§u sáº½ hÆ¡i lÃ¢u)...")
        # Sá»­ dá»¥ng model paraphrase-multilingual-MiniLM-L12-v2 há»— trá»£ tiáº¿ng Viá»‡t tá»‘t
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.vector_db = None
        
        # 2. Khá»Ÿi táº¡o LLM vá»›i Gemini 2.5 Flash
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.3,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
        
        # 3. Load bá»™ nhá»› cÅ© náº¿u Ä‘Ã£ tá»«ng há»c
        self._load_db()

    def _load_db(self):
        """HÃ m load Vector DB tá»« á»• cá»©ng lÃªn RAM"""
        if os.path.exists(os.path.join(VECTOR_DB_PATH, INDEX_NAME)):
            try:
                self.vector_db = FAISS.load_local(
                    os.path.join(VECTOR_DB_PATH, INDEX_NAME), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("âœ… ÄÃ£ load dá»¯ liá»‡u tri thá»©c cÅ©")
            except Exception as e:
                print(f"âŒ Lá»—i load DB: {e}")
        else:
            print("ğŸ“š ChÆ°a cÃ³ dá»¯ liá»‡u tri thá»©c - Äang tá»± Ä‘á»™ng load PDF...")
            self._auto_load_pdfs()
    
    def _auto_load_pdfs(self):
        """Tá»± Ä‘á»™ng load táº¥t cáº£ PDF cÃ³ sáºµn vÃ o vector database"""
        pdf_dir = os.path.join(BASE_DIR, "storage", "pdfs")
        if not os.path.exists(pdf_dir):
            print("âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c storage/pdfs")
            return
        
        pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]
        if not pdf_files:
            print("ğŸ“ KhÃ´ng cÃ³ file PDF nÃ o trong storage/pdfs")
            return
        
        print(f"ğŸ” TÃ¬m tháº¥y {len(pdf_files)} file PDF, Ä‘ang tá»± Ä‘á»™ng náº¡p...")
        total_chunks = 0
        for pdf_file in pdf_files:
            pdf_path = os.path.join(pdf_dir, pdf_file)
            try:
                chunks = self.ingest_pdf(pdf_path)
                total_chunks += chunks
                print(f"  âœ… {pdf_file}: {chunks} chunks")
            except Exception as e:
                print(f"  âŒ {pdf_file}: Lá»—i - {str(e)}")
        
        print(f"ğŸ‰ ÄÃ£ auto-load {total_chunks} chunks tá»« {len(pdf_files)} PDFs!")

    def ingest_pdf(self, file_path: str):
        """
        HÃ m Ä‘á»c file PDF vÃ  náº¡p vÃ o bá»™ nhá»›
        Sá»­ dá»¥ng PyPDFLoader Ä‘Æ¡n giáº£n vÃ  á»•n Ä‘á»‹nh
        """
        print(f"ğŸ“– Äang xá»­ lÃ½ file: {file_path}")
        
        try:
            # DÃ¹ng PyPDFLoader - Ä‘Æ¡n giáº£n, á»•n Ä‘á»‹nh
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            
            if not docs or len(docs) == 0:
                print("âŒ KhÃ´ng thá»ƒ Ä‘á»c ná»™i dung PDF")
                return 0
            
            print(f"ğŸ“„ ÄÃ£ Ä‘á»c {len(docs)} trang tá»« PDF")
            
            # DEBUG: Check if docs have actual text content
            total_text_length = sum(len(doc.page_content.strip()) for doc in docs)
            print(f"ğŸ” DEBUG: Tá»•ng Ä‘á»™ dÃ i text: {total_text_length} kÃ½ tá»±")
            
            if total_text_length < 10:
                print("âš ï¸ PDF cÃ³ thá»ƒ lÃ  scan/image, khÃ´ng cÃ³ text layer. Cáº§n OCR!")
                return 0
            
            # Cáº¯t nhá» vÄƒn báº£n (Chunking)
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200
            )
            chunks = splitter.split_documents(docs)
            
            print(f"ğŸ” DEBUG: Sá»‘ chunks sau split: {len(chunks) if chunks else 0}")
            
            if not chunks or len(chunks) == 0:
                print("âŒ KhÃ´ng cÃ³ ná»™i dung sau chunking")
                return 0

            # LÆ°u vÃ o Vector DB (FAISS)
            if self.vector_db:
                self.vector_db.add_documents(chunks)
            else:
                self.vector_db = FAISS.from_documents(chunks, self.embeddings)
                
            # LÆ°u xuá»‘ng á»• cá»©ng
            if not os.path.exists(VECTOR_DB_PATH):
                os.makedirs(VECTOR_DB_PATH)
                
            self.vector_db.save_local(os.path.join(VECTOR_DB_PATH, INDEX_NAME))
            print(f"âœ… ÄÃ£ há»c xong {len(chunks)} Ä‘oáº¡n kiáº¿n thá»©c")
            return len(chunks)
            
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ PDF: {e}")
            return 0

    def ask(self, symptoms: str, use_vision: bool = False):
        """
        HÃ m cháº©n Ä‘oÃ¡n bá»‡nh
        
        Args:
            symptoms: Triá»‡u chá»©ng cá»§a bá»‡nh nhÃ¢n
            use_vision: CÃ³ sá»­ dá»¥ng vision model khÃ´ng (cho áº£nh)
        """
        if not self.vector_db:
            return "Xin lá»—i, tÃ´i chÆ°a Ä‘Æ°á»£c há»c tÃ i liá»‡u nÃ o cáº£. Vui lÃ²ng upload sÃ¡ch PDF trÆ°á»›c."

        # 2. Táº¡o Prompt (NhÃ¢n cÃ¡ch bÃ¡c sÄ© ÄÃ´ng Y)
        prompt = ChatPromptTemplate.from_template("""
            Báº¡n lÃ  má»™t BÃ¡c sÄ© ÄÃ´ng Y (LÆ°Æ¡ng y) thÃ¢m niÃªn, uy tÃ­n vÃ  táº­n tÃ¢m.
            Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  há»— trá»£ cháº©n Ä‘oÃ¡n dá»±a trÃªn tÃ i liá»‡u y vÄƒn Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y.

            <TÃ i liá»‡u tham kháº£o>
            {context}
            </TÃ i liá»‡u tham kháº£o>
            
            Bá»‡nh nhÃ¢n mÃ´ táº£ triá»‡u chá»©ng: "{input}"
            
            HÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i chi tiáº¿t theo cáº¥u trÃºc sau:
            1. **Cháº©n Ä‘oÃ¡n sÆ¡ bá»™**: TÃªn bá»‡nh danh, BÃ¡t cÆ°Æ¡ng (HÃ n/Nhiá»‡t, HÆ°/Thá»±c...).
            2. **Biá»‡n chá»©ng luáº­n trá»‹**: Giáº£i thÃ­ch nguyÃªn nhÃ¢n táº¡i sao bá»‡nh nhÃ¢n bá»‹ nhÆ° váº­y dá»±a trÃªn táº¡ng phá»§.
            3. **PhÃ¡p trá»‹ & PhÆ°Æ¡ng dÆ°á»£c**: Äá» xuáº¥t bÃ i thuá»‘c (nÃªu rÃµ cÃ¡c vá»‹ thuá»‘c náº¿u cÃ³ trong tÃ i liá»‡u).
            4. **Lá»i khuyÃªn**: Cháº¿ Ä‘á»™ Äƒn uá»‘ng, sinh hoáº¡t.

            Náº¿u tÃ i liá»‡u khÃ´ng cÃ³ thÃ´ng tin vá» triá»‡u chá»©ng nÃ y, hÃ£y nÃ³i trung thá»±c: "Xin lá»—i, trong cÃ¡c sÃ¡ch tÃ´i Ä‘Ã£ há»c chÆ°a cÃ³ thÃ´ng tin vá» triá»‡u chá»©ng nÃ y."
        """)

        # 3. Táº¡o chuá»—i xá»­ lÃ½ (Chain)
        # Retriever tÃ¬m 5 Ä‘oáº¡n vÄƒn báº£n giá»‘ng nháº¥t trong sÃ¡ch
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
        
        # Káº¿t há»£p LLM + Prompt + Retriever
        chain = create_retrieval_chain(
            retriever, 
            create_stuff_documents_chain(self.llm, prompt)
        )
        
        # 4. Cháº¡y vÃ  tráº£ vá» káº¿t quáº£
        res = chain.invoke({"input": symptoms})
        return res["answer"]
    
    def chat(self, user_input: str):
        """
        HÃ m chat vá»›i ngÆ°á»i dÃ¹ng, tham kháº£o kiáº¿n thá»©c tá»« Vector DB
        Tráº£ vá» cÃ¢u tráº£ lá»i + tÃ i liá»‡u tham kháº£o
        """
        if not self.vector_db:
            return {
                "answer": "Xin lá»—i, tÃ´i chÆ°a Ä‘Æ°á»£c há»c tÃ i liá»‡u nÃ o. Vui lÃ²ng upload PDF trÆ°á»›c.",
                "sources": []
            }
        
        # 1. TÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
        relevant_docs = retriever.invoke(user_input)  # Updated method
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # 2. Táº¡o prompt
        prompt = ChatPromptTemplate.from_template("""
            Báº¡n lÃ  BÃ¡c sÄ© ÄÃ´ng Y chuyÃªn nghiá»‡p vá»›i kiáº¿n thá»©c sÃ¢u rá»™ng.
            
            <TÃ i liá»‡u tham kháº£o>
            {context}
            </TÃ i liá»‡u tham kháº£o>
            
            CÃ¢u há»i: "{input}"
            
            HÃ£y tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u tham kháº£o á»Ÿ trÃªn. Náº¿u tÃ i liá»‡u cÃ³ thÃ´ng tin liÃªn quan, hÃ£y:
            1. Liá»‡t kÃª cÃ¡c bá»‡nh cÃ³ thá»ƒ gáº·p
            2. Äá» xuáº¥t phÃ¡c Ä‘á»“ Ä‘iá»u trá»‹, bÃ i thuá»‘c (náº¿u cÃ³)
            3. ÄÆ°a ra lá»i khuyÃªn vá» cháº¿ Ä‘á»™ Äƒn uá»‘ng, sinh hoáº¡t
            
            Náº¿u tÃ i liá»‡u HOÃ€N TOÃ€N khÃ´ng liÃªn quan Ä‘áº¿n cÃ¢u há»i, hÃ£y nÃ³i: "Xin lá»—i, tÃ´i chÆ°a cÃ³ thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong tÃ i liá»‡u."
        """)
        
        # 3. Chain
        chain = (
            {"context": lambda x: context, "input": lambda x: x}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        answer = chain.invoke(user_input)
        
        # 4. Extract sources from metadata
        sources = []
        for doc in relevant_docs:
            if hasattr(doc, 'metadata') and 'source' in doc.metadata:
                source_path = doc.metadata['source']
                # Get filename from path
                filename = os.path.basename(source_path)
                if filename not in sources:
                    sources.append(filename)
        
        return {
            "answer": answer,
            "sources": sources
        }